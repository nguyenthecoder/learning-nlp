{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from os import listdir\n",
    "import sys\n",
    "from collections import Counter\n",
    "import string\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file):\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        return text\n",
    "\n",
    "def clean_doc(text):\n",
    "    # text = text.lower()\n",
    "    token = text.split(\" \")\n",
    "\n",
    "    #remove punctuations\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    token = [word.translate(table) for word in token]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #remove stopwords and non-alphabet character\n",
    "    # token = [word for word in token if word.isalpha() and word not in stop_words ]\n",
    "    token = [word for word in token if word.isalpha() and word not in stop_words]\n",
    "    # token = [word for word in token if word not in stop_words]\n",
    "\n",
    "\n",
    "    return token\n",
    "\n",
    "def generate_vocab(directory, is_train = True):\n",
    "    vocab = Counter()\n",
    "    for file in listdir(directory):\n",
    "        if is_train and file.startswith(\"cv9\"):\n",
    "            continue\n",
    "        if not is_train and not file.startswith(\"cv9\"):\n",
    "            continue\n",
    "        path = \"/\".join([directory, file])\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "#Load the word1vec file \n",
    "def load_embedding(filename, is_glove = False):\n",
    "    embedding = dict() \n",
    "    with open(filename) as file:\n",
    "        if is_glove:\n",
    "            lines = file.readlines()#glove format doesn't have header file so take all lines\n",
    "        else:\n",
    "            lines = file.readlines()[1:]# ignore the first line, which is 25435, 100 ( vocab_size, vector_size)\n",
    "        for line in lines:\n",
    "            parts = line.split(\" \")\n",
    "            embedding[parts[0]] = array(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "def embedding_to_weight_matrix(embedding, vocab, vector_size = 100):\n",
    "\n",
    "    # vocab size + 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1 \n",
    "\n",
    "    matrix = np.zeros((vocab_size, vector_size)) \n",
    "\n",
    "    for word, index in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            matrix[index] = vector \n",
    "\n",
    "    return matrix\n",
    "\n",
    "def process_features(features, labels, tokenizer, maxlen, is_train = True):\n",
    "    if is_train:\n",
    "        tokenizer.fit_on_texts(features)\n",
    "    encoded_dataset = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "    #pad sequences to the max length sequence\n",
    "    x_train = pad_sequences(encoded_dataset, maxlen=maxlen, padding=\"post\")\n",
    "    y_train = array(labels)\n",
    "    return x_train, y_train\n",
    "\n",
    "def run():\n",
    "    vocab1 = generate_vocab('./datasets/movie_review/txt_sentoken/neg')\n",
    "    vocab2 = generate_vocab('./datasets/movie_review/txt_sentoken/pos')\n",
    "\n",
    "    vocab = vocab1 + vocab2\n",
    "\n",
    "    print('Most common words ', vocab.most_common(10))\n",
    "\n",
    "    #Produce total tokens, ignore tokesn with count < 2\n",
    "    tokens = [word for word,count in vocab.items() if count >= 2]\n",
    "    print(len(tokens))\n",
    "\n",
    "    #save tokens to a file\n",
    "\n",
    "    with open('vocab.txt', 'w') as file:\n",
    "        data = \"\\n\".join(tokens)\n",
    "        file.write(data)  \n",
    "# run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  1800\n",
      "test dataset:  200\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset, label, vocab, is_training = True):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for file in listdir(dataset):\n",
    "        if is_training and file.startswith(\"cv9\"):\n",
    "            continue\n",
    "        if is_training == False and file.startswith(\"cv9\") == False:\n",
    "            continue\n",
    "        path = '/'.join([dataset, file])\n",
    "        text = load_doc(path)\n",
    "\n",
    "        tokens = clean_doc(text)\n",
    "        tokens = [word for word in tokens if word in vocab ]\n",
    "\n",
    "        x.append(tokens)\n",
    "        y.append(label)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "vocab = load_doc('vocab.txt')\n",
    "\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "positives, y_pos = load_dataset('./datasets/movie_review/txt_sentoken/pos', 0, vocab)\n",
    "negatives, y_neg = load_dataset('./datasets/movie_review/txt_sentoken/neg', 1, vocab)\n",
    "\n",
    "test_positives, test_y_pos = load_dataset('./datasets/movie_review/txt_sentoken/pos', 0, vocab, False)\n",
    "test_negatives, test_y_neg = load_dataset('./datasets/movie_review/txt_sentoken/neg', 1, vocab, False)\n",
    "\n",
    "training_dataset = positives + negatives\n",
    "training_labels = y_pos + y_neg\n",
    "print('training dataset: ', len(training_dataset))\n",
    "\n",
    "test_dataset = test_positives + test_negatives\n",
    "test_labels = test_y_pos + test_y_neg\n",
    "print('test dataset: ', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n",
      "training size:  1800\n",
      "training feature sample:  [   9 2952  238 ...    0    0    0]\n",
      "training label sample: 0\n",
      "test size:  200\n",
      "test feature sample:  [ 449  159 9140 ...    0    0    0]\n",
      "test label sample: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "max_len_token = max([len(s) for s in training_dataset]) \n",
    "print(max_len_token)\n",
    "x_train, y_train = process_features(training_dataset, training_labels, tokenizer, max_len_token)\n",
    "print('training size: ', len(x_train))\n",
    "print('training feature sample: ', x_train[0])\n",
    "print('training label sample:', y_train[0])\n",
    "\n",
    "x_test, y_test = process_features(test_dataset, test_labels, tokenizer, max_len_token, is_train=False)\n",
    "print('test size: ', len(x_test))\n",
    "print('test feature sample: ', x_test[0])\n",
    "print('test label sample:', y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostic(history, path = ''):\n",
    "    pyplot.figure(figsize=(8,8))\n",
    "    pyplot.figtext(0.5, 1, path, ha='center', fontsize=18)\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title(\"Loss\")\n",
    "    pyplot.plot(history.history['loss'], color=\"blue\", label=\"train loss\")\n",
    "    pyplot.plot(history.history['val_loss'], color=\"red\", label=\"validation loss\")\n",
    "    pyplot.legend()\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title(\"Accuracy\")\n",
    "    pyplot.plot(history.history['accuracy'], color=\"blue\", label=\"train accuracy\")\n",
    "    pyplot.plot(history.history['val_accuracy'], color=\"red\", label=\"validation accuracy\")\n",
    "    pyplot.legend()\n",
    "    if(path!= ''):\n",
    "        pyplot.savefig(path)\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def training_routine(model, args):\n",
    "    return model.fit(\n",
    "        args['x'],\n",
    "        args['y'],\n",
    "        epochs=args['epoch'],\n",
    "        verbose = args['verbose']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1291, 100)         2543600   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1284, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 642, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 20544)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               2054500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,623,833\n",
      "Trainable params: 4,623,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.6912 - accuracy: 0.5400\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.3680 - accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0220 - accuracy: 0.9972\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 6.9490e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 4.9079e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 3.1054e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 2.2093e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 1.6802e-04 - accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4056 - accuracy: 0.8650\n",
      "Test accuracy  0.8650000095367432 . Test loss  0.40558505058288574\n"
     ]
    }
   ],
   "source": [
    "def basic_model(vocab_size, max_length_token):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length_token))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = basic_model(len(tokenizer.word_index) + 1, max_len_token)\n",
    "model.summary()\n",
    "model = compile_model(model)\n",
    "\n",
    "history = training_routine(model, {\n",
    "    'x': x_train,\n",
    "    'y': y_train,\n",
    "    'epoch': 10,\n",
    "    'verbose': 1\n",
    "})\n",
    "\n",
    "# summarize_diagnostic(history)\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy ', acc, '. Test loss ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding layer with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25435\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.45786524  0.54656488  0.05135894 ... -0.76674253  0.59616333\n",
      "   0.18149421]\n",
      " [-0.5772382   0.81545228  0.08127926 ... -0.75325519  0.21177313\n",
      "   0.22589795]\n",
      " ...\n",
      " [-0.00179134  0.02266374  0.01850034 ... -0.0237722   0.02090832\n",
      "   0.02317982]\n",
      " [ 0.00094989  0.02554806  0.00635505 ... -0.00704258  0.01889366\n",
      "   0.01335397]\n",
      " [-0.00352503  0.02271733  0.00564215 ... -0.01373077  0.00766795\n",
      "   0.00602894]]\n"
     ]
    }
   ],
   "source": [
    "#Load trained word 2 vector\n",
    "embedding = load_embedding('word_2_vec.txt')\n",
    "\n",
    "weight_matrix = embedding_to_weight_matrix(embedding, tokenizer.word_index )\n",
    "print(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19915999 -0.049702    0.24579    ... -0.068109    0.017651\n",
      "   0.06455   ]\n",
      " [ 0.38251001  0.14821     0.60601002 ...  0.058921    0.091112\n",
      "   0.47283   ]\n",
      " ...\n",
      " [-0.078809   -0.73105001 -0.12292    ... -0.63562    -0.48644999\n",
      "   0.094265  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12185    -0.45809001 -0.043794   ...  0.37827    -0.28586999\n",
      "   0.3994    ]]\n"
     ]
    }
   ],
   "source": [
    "glove = load_embedding('./datasets/glove.6B.100d.txt', is_glove=True)\n",
    "glove_weight_matrix = embedding_to_weight_matrix(glove, tokenizer.word_index )\n",
    "print(glove_weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 8s 11ms/step - loss: 0.7508 - accuracy: 0.5178\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6758 - accuracy: 0.5856\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6173 - accuracy: 0.6672\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.5330 - accuracy: 0.7411\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.4435 - accuracy: 0.7928\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.3479 - accuracy: 0.8728\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.2582 - accuracy: 0.9078\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.1833 - accuracy: 0.9544\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.1235 - accuracy: 0.9794\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0817 - accuracy: 0.9922\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4315 - accuracy: 0.5250\n",
      "Test accuracy  0.5249999761581421 . Test loss  1.4315271377563477\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_w2v_model(weight_matrix, is_embedding_trainable = False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(vocab) + 1, 100, weights=[weight_matrix], input_length=max_len_token, trainable=is_embedding_trainable ))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = get_embedding_w2v_model(weight_matrix)\n",
    "model = compile_model(model)\n",
    "\n",
    "history = training_routine(model, {\n",
    "    'x': x_train,\n",
    "    'y': y_train,\n",
    "    'epoch': 10,\n",
    "    'verbose': 1\n",
    "})\n",
    "\n",
    "# summarize_diagnostic(history)\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy ', acc, '. Test loss ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pretrained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 3s 30ms/step - loss: 0.8434 - accuracy: 0.5022\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.6234 - accuracy: 0.7700\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.4228 - accuracy: 0.8383\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1647 - accuracy: 0.9728\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0456 - accuracy: 0.9983\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5301 - accuracy: 0.8000\n",
      "Test accuracy  0.800000011920929 . Test loss  0.5300505757331848\n"
     ]
    }
   ],
   "source": [
    "model = get_embedding_w2v_model(glove_weight_matrix, True)\n",
    "\n",
    "model = compile_model(model)\n",
    "\n",
    "history = training_routine(model, {\n",
    "    'x': x_train,\n",
    "    'y': y_train,\n",
    "    'epoch': 10,\n",
    "    'verbose': 1\n",
    "})\n",
    "\n",
    "# summarize_diagnostic(history)\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy ', acc, '. Test loss ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd4b50b73317022a6c6cd77d285e17e0ffb874fa93a50956528cb9bde22a05db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
