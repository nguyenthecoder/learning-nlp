{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: Given a setence, classify if it's Robert Frost or Edgar Allen Poe poem  \n",
    "\n",
    "steps:\n",
    "1. get the data\n",
    "2. process data\n",
    "3. save labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python  -m wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "# !python -m wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    line = re.sub(\"[^a-zA-Z ]\", '', line)\n",
    "    return line\n",
    "\n",
    "def tolkenizer(sentences):\n",
    "    wordMap = {'<unknown>': 0}\n",
    "    count =1 \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split(\" \")\n",
    "        for word in words:\n",
    "            if(word in wordMap):\n",
    "                continue\n",
    "            else:\n",
    "                wordMap[word] = count\n",
    "                count = count + 1\n",
    "    return wordMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samples : 2378\n",
      "class robert frost (0) 797\n",
      "class allan poe (1) 1581\n"
     ]
    }
   ],
   "source": [
    "features = list()\n",
    "labels = list()\n",
    "class0 = 0\n",
    "\n",
    "with open('edgar_allan_poe.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    count0 = len(lines)\n",
    "    for line in lines:\n",
    "        line = clean(line)\n",
    "        features.append(line)\n",
    "        labels.append(0)\n",
    "        class0 = class0 + 1\n",
    "\n",
    "class1 = 0\n",
    "with open('robert_frost.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        line = clean(line)\n",
    "        features.append(line)\n",
    "        labels.append(1)\n",
    "        class1 = class1 + 1\n",
    "\n",
    "print('Dataset samples : ' + str(len(features)))\n",
    "print('class robert frost (0) ' + str(class0))\n",
    "print('class allan poe (1) ' + str(class1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but everybody took it for proof', 'it is not that my founts of bliss', '', 'to where it bent in the undergrowth', 'this was no playhouse but a house in earnest']\n",
      "[1, 0, 1, 1, 1]\n",
      "word map lenght : 2534\n",
      "[[83, 362, 341, 1, 104, 363], [1, 40, 72, 143, 254, 364, 8, 365], [102], [20, 170, 1, 366, 12, 5, 367], [263, 2, 159, 368, 83, 46, 172, 12, 369]]\n",
      "[1, 0, 1, 1, 1]\n",
      "[[17, 21, 154, 49, 433, 10, 5, 258], [28, 125, 127, 336, 5, 588, 8, 5, 980], [20, 0, 5, 661, 28, 478, 5, 691], [204, 137, 915, 5, 330, 351, 682, 683], [28, 17, 62, 61, 40, 1778, 52, 1779]]\n",
      "[1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels)\n",
    "print(x_train[100: 105])\n",
    "print(y_train[100: 105])\n",
    "\n",
    "wordMap = tolkenizer(x_train)\n",
    "\n",
    "print(\"word map lenght :\", len(wordMap))\n",
    "# Convert sentence to idx\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = [wordMap[word] for word in x_train[i].split(\" \")]\n",
    "\n",
    "# Convert test \n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = [wordMap.get(word, 0) for word in x_test[i].split(\" \")]\n",
    "\n",
    "print(x_train[100: 105])\n",
    "print(y_train[100: 105])\n",
    "\n",
    "print(x_test[100: 105])\n",
    "print(y_test[100: 105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0\n",
      "Max =  0.010093457943925233\n",
      "Min =  0.0003620564808110065\n",
      "Max =  0.011290929619872036\n",
      "Min =  0.0003399048266485384\n",
      "Prior 0 =  0.3370723499719574 , prior 1= 0.6629276500280427\n",
      "log prior 0 =  -1.0874576833279328 , log prior 1 =  -0.41108941989709685\n"
     ]
    }
   ],
   "source": [
    "M = len(wordMap)\n",
    "A0 = np.ones((M, M))\n",
    "pi0 = np.ones(M)\n",
    "\n",
    "A1 = np.ones((M,M))\n",
    "pi1 = np.ones(M)\n",
    "\n",
    "def count_occurences(x_train, A, pi):\n",
    "    for x in x_train:\n",
    "        prev = None\n",
    "        for idx in x:\n",
    "            if prev == None:\n",
    "                pi[idx] += 1\n",
    "            else:\n",
    "                A[prev, idx] += 1\n",
    "            prev = idx\n",
    "\n",
    "count_occurences([t for t,y in zip(x_train, y_train) if y == 0 ] , A0, pi0)\n",
    "count_occurences([t for t,y in zip(x_train, y_train) if y == 1 ] , A1, pi1)\n",
    "\n",
    "min = np.min(A0)\n",
    "\n",
    "max = np.max(A0)\n",
    "print(max)\n",
    "\n",
    "# Normalize A, pi\n",
    "A0 /= A0.sum(axis = 1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "print('Max = ', np.max(A0))\n",
    "print('Min = ', np.min(A0))\n",
    "\n",
    "A1 /= A1.sum(axis = 1, keepdims= True)\n",
    "pi1 /= pi1.sum()\n",
    "\n",
    "print('Max = ', np.max(A1))\n",
    "print('Min = ', np.min(A1))\n",
    "\n",
    "logA0 = np.log(A0)\n",
    "logA1 = np.log(A1)\n",
    "\n",
    "log_pi0= np.log(pi0)\n",
    "log_pi1 = np.log(pi1)\n",
    "\n",
    "prior0 = sum(y == 0 for y in y_train) / len(y_train)\n",
    "prior1 = sum(y == 1 for y in y_train) / len(y_train)\n",
    "\n",
    "log_prior0 = np.log(prior0)\n",
    "log_prior1 = np.log(prior1)\n",
    "\n",
    "print(\"Prior 0 = \", prior0, \", prior 1=\" ,prior1)\n",
    "print(\"log prior 0 = \", log_prior0, \", log prior 1 = \", log_prior1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, logAs, logPis, logPs):\n",
    "        self.logAs = logAs\n",
    "        self.logPis = logPis\n",
    "        self.logPs = logPs\n",
    "        self.K = len(logPs)\n",
    "\n",
    "    def compute_probability(self, input, class_):\n",
    "        logA = self.logAs[class_]\n",
    "        logPi = self.logPis[class_]\n",
    "\n",
    "        last_idx = None\n",
    "        logProb = 0\n",
    "\n",
    "        for idx in input:\n",
    "            if(last_idx == None):\n",
    "                logProb += logPi[idx]\n",
    "            else:\n",
    "                logProb += logA[last_idx, idx]\n",
    "            last_idx = idx\n",
    "\n",
    "        return logProb\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "\n",
    "        preds = np.zeros(len(inputs))\n",
    "\n",
    "        for i, input in enumerate(inputs):\n",
    "            posteriors = [self.compute_probability(input, c) + self.logPs[c] for c in range(self.K)]\n",
    "        \n",
    "            pred = np.argmax(posteriors)\n",
    "            preds[i] = pred\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.9607403252944475\n",
      "Train acc:  0.8084033613445378\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier([logA0, logA1], [log_pi0, log_pi1], [log_prior0, log_prior1])\n",
    "\n",
    "ptrain = clf.predict(x_train)\n",
    "print(\"Train acc: \", np.mean(ptrain == y_train))\n",
    "\n",
    "# ptest = clf.predict(x_test)\n",
    "ptest = clf.predict(x_test)\n",
    "print(\"Train acc: \", np.mean(ptest == y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 531   70]\n",
      " [   0 1182]]\n",
      "[[ 98  98]\n",
      " [ 16 383]]\n"
     ]
    }
   ],
   "source": [
    "cm_train= confusion_matrix(y_train, ptrain)\n",
    "print(cm_train)\n",
    "\n",
    "cm_test= confusion_matrix(y_test, ptest)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 training  0.9712407559572719\n",
      "f1 testing  0.8704545454545454\n"
     ]
    }
   ],
   "source": [
    "print( 'f1 training ', f1_score(y_train, ptrain))\n",
    "print( 'f1 testing ', f1_score(y_test, ptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd4b50b73317022a6c6cd77d285e17e0ffb874fa93a50956528cb9bde22a05db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
